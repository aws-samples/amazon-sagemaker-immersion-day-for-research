{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad61b266-703b-48f5-abf1-cf6a4b315ff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 🧑‍🏫 Large Language Models for Education 🧑‍🏫\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 🧑‍🎓 A note on Generative AI in Education\n",
    "\n",
    "By harnessing generative AI, educators can unlock new and captivating products, enabling them to craft engaging and interactive learning experiences that promote student growth. Experts envision a future where generative AI empowers educators to revolutionize the way knowledge is imparted, paving the way for transformative educational practices.\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, we demonstrate how to use large language models (LLMs) for use cases in education.  LLMs can be used for tasks such as summarization, question-answering, or the generation of question & answer pairs.\n",
    "\n",
    "Text Summarization is the task of shortening the data and creating a summary that represents the most important information present in the original text. Here, we show how to use state-of-the-art pre-trained model **Llama-2-7b-chat** for text summarization, as well all the other tasks. \n",
    "\n",
    "In the first part of the notebook, we select and deploy the **Llama-2-7b-chat** model as a SageMaker Real-time endpoint, on a single `ml.g5.2xlarge` instance. SageMaker Real-time endpoints is ideal for inference workloads where you have real-time, interactive, low latency requirements.  These endpoints are fully managed, automatically serve your models through HTTP, and support auto-scaling.\n",
    "\n",
    "Once the model is deployed and ready to use, we demonstrate how it can be queried, how to prompt the model for summarization, question-answering and the generation of question & answer pairs.\n",
    "\n",
    "The final section is split into four demos on querying the Wikipedia article on quantum computing, an ebook text from [Project Gutenberg](https://www.gutenberg.org/), a scientific pdf article from arxiv.org, and the Australian Budget 2023-24 Medicare Overview."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882bdb91-fa4a-43d1-90e4-52a319b57743",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1.Setting up the SageMaker Endpoint\n",
    "\n",
    "### 1.1 Install Python Dependencies and SageMaker setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5805af2-10cd-4ad3-af9d-0b863b323f54",
   "metadata": {
    "tags": []
   },
   "source": [
    "Before executing the notebook, there are some initial steps required for set up. This notebook requires latest version of sagemaker and other libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62662a0-fa82-475c-b671-2ebeb67467ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install -U sagemaker\n",
    "!pip install -U langchain\n",
    "!pip install -U PyPDF2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4afab6b-5f54-4ccd-b173-3670dc2ecd99",
   "metadata": {},
   "source": [
    "We Load SDK and helper scripts. First, we import required packages and load the S3 bucket from SageMaker session, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "011c244c-ca72-40f2-b7c9-7a94f25becf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json, logging\n",
    "from sagemaker import image_uris, instance_types, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.utils import name_from_base\n",
    "from IPython.display import display, HTML, IFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13b46906-f59b-439e-b10a-cc65725177dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logger = logging.getLogger('sagemaker')\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(logging.StreamHandler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20233a5f-08ce-4d04-bae4-379c32e9f0af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using sagemaker==2.183.0\n",
      "Using boto3==1.28.63\n"
     ]
    }
   ],
   "source": [
    "logger.info(f'Using sagemaker=={sagemaker.__version__}')\n",
    "logger.info(f'Using boto3=={boto3.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae27a1cd-6507-4a12-acfa-a4d2cf6289e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the folder where the model weights will be stored\n",
    "!mkdir -p download_dir\n",
    "!mkdir -p source_documents_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "747d0e08-797c-4e34-8db0-274d9ab58d89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_sagemaker_session(local_download_dir) -> sagemaker.Session:\n",
    "    \"\"\"Return the SageMaker session.\"\"\"\n",
    "\n",
    "    sagemaker_client = boto3.client(\n",
    "        service_name=\"sagemaker\", region_name=boto3.Session().region_name\n",
    "    )\n",
    "\n",
    "    session_settings = sagemaker.session_settings.SessionSettings(\n",
    "        local_download_dir=local_download_dir\n",
    "    )\n",
    "\n",
    "    # the unit test will ensure you do not commit this change\n",
    "    session = sagemaker.session.Session(\n",
    "        sagemaker_client=sagemaker_client, settings=session_settings\n",
    "    )\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e356a656-ae9e-44fc-857c-1f717012514e",
   "metadata": {},
   "source": [
    "### 1.2 Deploying a SageMaker Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5a8649-2aac-4ab5-b4b1-c3af59893dde",
   "metadata": {},
   "source": [
    "See the 'Set Up Llama-2-7b-chat endpoint'section in the 'Readme' file in this directory deployment instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa55310f-fab2-40d8-aa5a-0e2b2588adb0",
   "metadata": {},
   "source": [
    "## 3. LLM Demos for Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81be0382-3cd1-4707-baf2-8752ac46ef62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nlp_helper\n",
    "#nlp_helper.endpoint_name = 'jumpstart-dft-meta-textgeneration-llama-2-7b-f'\n",
    "endpoint = 'jumpstart-dft-meta-textgeneration-llama-2-7b-chat'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad5a23-488d-4b9f-8d2e-7a07130a9a2c",
   "metadata": {
    "tags": []
   },
   "source": [
    "In this notebook, we use the following texts to demonstrate summarization tasks and the generation of question & answer pairs.\n",
    "\n",
    "1. Quantum Computing from Wikipedia: https://en.wikipedia.org/wiki/Quantum_computing\n",
    "<!-- 1. Quantum Computing and Quantum Information (by Nielsen & Chuang): https://michaelnielsen.org/qcqi/QINFO-book-nielsen-and-chuang-toc-and-chapter1-nov00.pdf (this is a sample chapter from [this website](https://michaelnielsen.org/qcqi/)) -->\n",
    "2. Winnie the Pooh (by Alan Alexander Milne): https://www.gutenberg.org/ebooks/67098.txt.utf-8\n",
    "3. Attention is all you need (by Vaswani et al): https://arxiv.org/pdf/1706.03762.pdf\n",
    "4. Australian Budget 2023-24 Overview: https://budget.gov.au/content/overview/download/budget_overview-20230511.pdf\n",
    "\n",
    "Note that for this notebook, we are using the Llama-2-7b-chat model for simplicity and ease of deployment--additional fine tuning or using improved models would be required to get better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d29ef7-870c-456a-8a58-b60c788de24b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Download pdfs and texts with the `curl` command. Flags used here are `-L` (allow redirects),\n",
    "# `-s` (for silent mode) and `-o` (to specify the output file name).\n",
    "\n",
    "# Attention is all you need (by Vaswani et al)\n",
    "!curl -Ls https://arxiv.org/pdf/1706.03762.pdf -o source_documents_dir/attention.pdf\n",
    "# Australian Budget 2023-24 Overview\n",
    "!curl -Ls https://budget.gov.au/content/overview/download/budget_overview-20230511.pdf -o source_documents_dir/aus_budget_overview-2023-24.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ee88e7-9ad6-4e04-add3-7d3842ed3c3c",
   "metadata": {},
   "source": [
    "### 3.1 Wikipedia Page on Quantum Computing\n",
    "\n",
    "In this example, a Wikipedia page on Quantum Computing is used for context. The LLM is used for keyword generation, a point by point summary, and a set of question and answer pairs. You may also wish to replace the Wikipedia URL with a website, blog, or news article of your own preference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72ea7b6f-6174-48ba-aa28-33ca600dcee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NCHARS = 400     # We will show just the first and last 400 characters of each extracted text. Increase this number for more context.\n",
    "NQUESTIONS = 10  # The number of Q&A pairs that we will generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "61f5f17c-bb54-4bdd-bf2c-2c63b4d9ae9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"300\"\n",
       "            src=\"https://en.wikipedia.org/wiki/Quantum_computing\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa40ec44e50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_paragraphs = nlp_helper.extract_paragraphs_from_html(\n",
    "    nlp_helper.download_url_text('https://en.wikipedia.org/wiki/Quantum_computing')\n",
    ")[1:11]  # We will skip the first 2 paragraphs\n",
    "wiki_txt = '\\n\\n'.join(wiki_paragraphs)\n",
    "IFrame('https://en.wikipedia.org/wiki/Quantum_computing', width=800, height=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "589cb4f5-0d37-4a40-98bc-fe0dd461f6ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A quantum computer is a computer that takes advantage of quantum mechanical phenomena.\n",
      "\n",
      "\n",
      "At small scales, physical matter exhibits properties of both particles and waves, and quantum computing leverages this behavior, specifically quantum superposition and entanglement, using specialized hardware that supports the preparation and manipulation of quantum states.\n",
      "\n",
      "\n",
      "Classical physics cannot explain the operation of these quantum devices, and a scalable quantum computer could perform some calculations exponentially faster than any modern \"classical\" computer. In particular, a large-scale quantum computer could break widely used encryption schemes and aid physicists in performing physical simulations; however, the current state of the art is largely experimental and impractical, with several obstacles to useful applications. Moreover, scalable quantum computers do not hold promise for many practical tasks, and for many important tasks quantum speedups are proven impossible.\n",
      "\n",
      "\n",
      "The basic unit of information in quantum computing is the qubit, similar to the bit in traditional digital electronics. Unlike a classical bit, a qubit can exist in a superposition of its two \"basis\" states, which loosely means that it is in both states simultaneously. When measuring a qubit, the result is a probabilistic output of a classical bit, therefore making quantum computers nondeterministic in general. If a quantum computer manipulates the qubit in a particular way, wave interference effects can amplify the desired measurement results. The design of quantum algorithms involves creating procedures that allow a quantum computer to perform calculations efficiently and quickly.\n",
      "\n",
      "\n",
      "Physically engineering high-quality qubits has proven challenging. If a physical qubit is not sufficiently isolated from its environment, it suffers from quantum decoherence, introducing noise into calculations. Paradoxically, perfectly isolating qubits is also undesirable because quantum computations typically need to initialize qubits, perform controlled qubit interactions, and measure the resulting quantum states. Each of those operations introduces errors and suffers from noise, and such inaccuracies accumulate.\n",
      "\n",
      "\n",
      "National governments have invested heavily in experimental research that aims to develop scalable qubits with longer coherence times and lower error rates. Two of the most promising technologies are superconductors (which isolate an electrical current by eliminating electrical resistance) and ion traps (which confine a single ion using electromagnetic fields).\n",
      "\n",
      "\n",
      "In principle, a non-quantum (classical) computer can solve the same computational problems as a quantum computer, given enough time. Quantum advantage comes in the form of time complexity rather than computability, and quantum complexity theory shows that some quantum algorithms for carefully selected tasks require exponentially fewer computational steps than the best known non-quantum algorithms. Such tasks can in theory be solved on a large-scale quantum computer whereas classical computers would not finish computations in any reasonable amount of time. However, quantum speedup is not universal or even typical across computational tasks, since basic tasks such as sorting are proven to not allow any asymptotic quantum speedup. Claims of quantum supremacy have drawn significant attention to the discipline, but are demonstrated on contrived tasks, while near-term practical use cases remain limited. \n",
      "\n",
      "\n",
      "Optimism about quantum computing is fueled by a broad range of new theoretical hardware possibilities facilitated by quantum physics, but the improving understanding of quantum computing limitations counterbalances this optimism. In particular, quantum speedups have been traditionally estimated for noiseless quantum computers, whereas the impact of noise and the use of quantum error-correction can undermine low-polynomial speedups.\n",
      "\n",
      "\n",
      "For many years, the fields of quantum mechanics and computer science formed distinct academic communities.[2] Modern quantum theory developed in the 1920s to explain the wave–particle duality observed at atomic scales,[3] and digital computers emerged in the following decades to replace human computers for tedious calculations.[4] Both disciplines had practical applications during World War II; computers played a major role in wartime cryptography,[5] and quantum physics was essential for the nuclear physics used in the Manhattan Project.[6]\n",
      "\n",
      "\n",
      "As physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, the fields of quantum mechanics and computer science began to converge.\n",
      "In 1980, Paul Benioff introduced the quantum Turing machine, which uses quantum theory to describe a simplified computer.[7]\n",
      "When digital computers became faster, physicists faced an exponential increase in overhead when simulating quantum dynamics,[8] prompting Yuri Manin and Richard Feynman to independently suggest that hardware based on quantum phenomena might be more efficient for computer simulation.[9][10][11]\n",
      "In a 1984 paper, Charles Bennett and Gilles Brassard applied quantum theory to cryptography protocols and demonstrated that quantum key distribution could enhance information security.[12][13]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wiki_txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95a891c-e574-4ea3-ac7f-0d01e52aa38e",
   "metadata": {},
   "source": [
    "#### Key word Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbfd80ba-7244-45b8-a18e-09affbf567cf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Based on the given context, here are the top 5 key words:\n",
      "\n",
      "1. Quantum computer\n",
      "2. Superposition\n",
      "3. Entanglement\n",
      "4. Qubit\n",
      "5. Cryptography\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.serializers import JSONSerializer\n",
    "sagemaker_session = Session()\n",
    "predictor = Predictor(endpoint_name = endpoint, sagemaker_session=sagemaker_session)\n",
    "content = f'FIND TOP 5 KEY WORDS IN FOLLOWING CONTEXT\\n\\nContext:\\n{wiki_txt}\\nKey Words:'\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.9} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "print(json.loads(response.decode())[0]['generation']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9859bd8-e743-4a34-817f-06405a06a2d4",
   "metadata": {},
   "source": [
    "#### Summary of key points\n",
    "\n",
    "For each of each of paragraphs, let's create a short summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe371a26-340a-41d3-9138-36479871dce8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "2.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "3.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "4.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "5.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "6.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "7.  I'm not sure I understand what you are saying with \"[\". Could you explain?\n",
      "\n",
      "8.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "9.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n",
      "10.  I'm not sure I understand what you are saying with \"[/]. Could you explain?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = []\n",
    "for i, x in enumerate(wiki_paragraphs):\n",
    "    content = f'Summarise the following information within 20 words:\\n{x[:1500]}\\nSummary:\\n'\n",
    "    payload = {\n",
    "        \"inputs\": [[\n",
    "            {\"role\": \"user\", \"content\": ''},\n",
    "        ]],\n",
    "        \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.1} \n",
    "    }\n",
    "    predictor.serializer = JSONSerializer()\n",
    "    response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "    summary_paragraph = json.loads(response.decode())[0]['generation']['content']\n",
    "    summary.append(f'{i+1}. {summary_paragraph}')\n",
    "    print(f'{i+1}. {summary_paragraph}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "284d81bd-fb49-46d6-8142-9a8b8393e504",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Key Points</h4><li>1.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>2.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>3.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>4.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>5.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>6.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>7.  I'm not sure I understand what you are saying with \"[\". Could you explain?</li>\n",
       "<li>8.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>9.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>\n",
       "<li>10.  I'm not sure I understand what you are saying with \"[/]. Could you explain?</li>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\n",
    "    '<h4>Key Points</h4>' + \n",
    "    '\\n'.join([ f'<li>{x}</li>' for x in summary ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84410c39-d71c-44f4-95d2-744cf527a9e1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Repeated requests for clarification of unclear statements.\n"
     ]
    }
   ],
   "source": [
    "content = f'Summarise the following information within 20 words:\\n{summary}\\nSummary:\\n'\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"user\", \"content\": content},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.7} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "summary_total = json.loads(response.decode())[0]['generation']['content']\n",
    "print(summary_total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aa90c0b-31e4-4286-9995-48ca4f66efad",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Checking for correct answers\n",
    "\n",
    "In this example, we generate a \"correct answer\" based on the text. One incorrect answer,\n",
    "and one correct answer (paraphrased slightly differently from the official \"correct answer\")\n",
    "from a student are generated. The LLM is used to check if the student's answer is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b40942c-d2fc-4121-8143-a5875d5d4f66",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Quantum computing is a field of study that leverages quantum mechanical phenomena to develop computers that can solve certain problems exponentially faster than classical computers.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Give an answer within 50 words: What is quantum computing?\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Alwaysd complete the tasks based on the context:{wiki_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c12861c0-991c-4690-8a2c-a6d655ab3742",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, that is not correct. Quantum computing is not about using computers with quantum dots. Quantum computing is a field of study that explores the use of quantum mechanics to develop computers that can solve certain problems faster than classical computers. Quantum computers use quantum bits, or qubits, which are quantum mechanical systems that can exist in multiple states simultaneously, to perform calculations. These qubits are manipulated using quantum gates to perform operations that are not possible with classical computers.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"\n",
    "Question: What is quantum computing?\n",
    "Answer within 50 words: {answer}\n",
    "Student: Quantum computing is using computers with quantum dots\n",
    "Is this answer correct?\"\"\"\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3d663433-8b29-4dfe-8e11-542c48ab919e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " No, that is not correct. Quantum computing is not about using computers that make use of quantum mechanics. Quantum computing is a field of study that explores the use of quantum mechanics to develop computers that can solve certain problems faster than classical computers. Quantum computers use quantum bits, or qubits, which are quantum mechanical systems that can exist in multiple states simultaneously, to perform calculations. These qubits are manipulated using quantum gates to perform operations that are not possible with classical computers.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"\n",
    "Question: What is quantum computing?\n",
    "Answer within 50 words: {answer}\n",
    "Student: Quantum computing involves using computers that make use of quantum mechanics\n",
    "Is this answer correct?\"\"\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab797e5-539b-49e4-8d8b-01113d7b7a04",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Generation of Question & Answer Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cd4876c-b5cb-498d-ac7f-0b0b52c415a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure! Here are 10 question and answer pairs based on the context:\n",
      "\n",
      "Question: What is a quantum computer?\n",
      "Answer: A quantum computer is a computer that takes advantage of quantum mechanical phenomena, such as superposition and entanglement, to perform calculations exponentially faster than classical computers.\n",
      "\n",
      "Question: Why can't classical physics explain the operation of quantum devices?\n",
      "Answer: Classical physics cannot explain the operation of quantum devices because quantum devices rely on quantum mechanical phenomena, which are not described by classical physics.\n",
      "\n",
      "Question: What is the basic unit of information in quantum computing?\n",
      "Answer: The basic unit of information in quantum computing is the qubit, which can exist in a superposition of its two basis states.\n",
      "\n",
      "Question: Why are quantum computers nondeterministic?\n",
      "Answer: Quantum computers are nondeterministic because the result of measuring a qubit is probabilistic, meaning that it is not guaranteed what the outcome will be.\n",
      "\n",
      "Question: What are some of the challenges in physically engineering high-quality qubits?\n",
      "Answer: Some of the challenges in physically engineering high-quality qubits include isolating them from their environment to prevent decoherence and introducing errors, and also initializing and manipulating qubits in a way that minimizes errors.\n",
      "\n",
      "Question: Can a classical computer solve the same computational problems as a quantum computer?\n",
      "Answer: In principle, a classical computer can solve the same computational problems as a quantum computer, given enough time, but quantum computers have the potential to solve some problems faster due to time complexity rather than computability.\n",
      "\n",
      "Question: What is quantum complexity theory?\n",
      "Answer: Quantum complexity theory is the study of the computational resources required to solve a problem using a quantum computer, and it shows that some quantum algorithms require exponentially fewer computational steps than the best known non-quantum algorithms for certain tasks.\n",
      "\n",
      "Question: What are some of the near-term practical use cases for quantum computing?\n",
      "Answer: Some of the near-term practical use cases for quantum computing include breaking widely used encryption schemes and performing physical simulations, but the current state of the art is largely experimental and impractical.\n",
      "\n",
      "Question: How did the fields of quantum mechanics and computer science converge?\n",
      "Answer: The fields of quantum mechanics and computer science converged as physicists applied quantum mechanical models to computational problems and swapped digital bits for qubits, leading to the development of quantum computing.\n",
      "\n",
      "Question: What is the quantum Turing machine?\n",
      "Answer: The quantum Turing machine is a theoretical model of a quantum computer that uses quantum theory to describe a simplified computer, introduced in 1980 by Paul Benioff.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"\n",
    "Create {NQUESTIONS} question and answer pairs in the format: \"Question: ... Answer: ...\" \"\"\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always complete the tasks based only on the context:{wiki_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 1000, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8b220a-7c2d-4d23-abe8-8ff0323b0e3c",
   "metadata": {},
   "source": [
    "### 3.2 Winnie the Pooh (by Alan Alexander Milne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "26672f79-120b-496f-897c-d4b603ee285e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "winnie_the_pooh = nlp_helper.download_url_text('https://www.gutenberg.org/ebooks/67098.txt.utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89f2bd54-248a-45b1-a882-2159f0a9e4df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHAPTER III\n",
      "\n",
      "                   IN WHICH POOH AND PIGLET GO HUNTING\n",
      "                        AND NEARLY CATCH A WOOZLE\n",
      "\n",
      "\n",
      "The Piglet lived in a very grand house in the middle of a beech-tree,\n",
      "and the beech-tree was in the middle of the forest, and the Piglet lived\n",
      "in the middle of the house. Next to his house was a piece of broken\n",
      "board which had: \"TRESPASSERS W\" on it. When Christopher Robin asked the\n",
      "Piglet what it meant, he said it was his grandfather's name, and had\n",
      "been in the family for a long time, Christopher Robin said you\n",
      "_couldn't_ be called Trespassers W, and Piglet said yes, you could,\n",
      "because his grandfather was, and it was short for Trespassers Will,\n",
      "which was short for Trespassers William. And his grandfather had had two\n",
      "names in case he lost one--Trespassers after an uncle, and William after\n",
      "Trespassers.\n",
      "\n",
      "\"I've got two names,\" said Christopher Robin carelessly.\n",
      "\n",
      "\"Well, there you are, that proves it,\" said Piglet.\n",
      "\n",
      "One fine winter's day when Piglet was brushing away the snow in front of\n",
      "his house, he happened to look up, and there was Winnie-the-Pooh. Pooh\n",
      "was walking round and round in a circle, thinking of something else, and\n",
      "when Piglet called to him, he just went on walking.\n",
      "\n",
      "\"Hallo!\" said Piglet, \"what are _you_ doing?\"\n",
      "\n",
      "\"Hunting,\" said Pooh.\n",
      "\n",
      "\"Hunting what?\"\n",
      "\n",
      "\"Tracking something,\" said Winnie-the-Pooh very mysteriously.\n",
      "\n",
      "\"Tracking what?\" said Piglet, coming closer.\n",
      "\n",
      "\"That's just what I ask myself. I ask myself, What?\"\n",
      "\n",
      "\"What do you think you'll answer?\"\n",
      "\n",
      "\"I shall have to wait until I catch up with it,\" said Winnie-the-Pooh.\n",
      "\"Now, look there.\" He pointed to the ground in front of him. \"What do\n",
      "you see there?\"\n",
      "\n",
      "\"Tracks,\" said Piglet. \"Paw-marks.\" He gave a little squeak of\n",
      "excitement. \"Oh, Pooh! Do you think it's a--a--a Woozle?\"\n",
      "\n",
      "\"It may be,\" said Pooh. \"Sometimes it is, and sometimes it isn't. You\n",
      "never can tell with paw-marks.\"\n",
      "\n",
      "With these few words he went on tracking, and Piglet, after watching him\n",
      "for a minute or two, ran after him. Winnie-the-Pooh had come to a sudden\n",
      "stop, and was bending over the tracks in a puzzled sort of way.\n",
      "\n",
      "\"What's the matter?\" asked Piglet.\n",
      "\n",
      "\"It's a very funny thing,\" said Bear, \"but there seem to be\n",
      "_two_ animals now. This--whatever-it-was--has been joined by\n",
      "another--whatever-it-is--and the two of them are now proceeding\n",
      "in company. Would you mind coming with me, Piglet, in case they\n",
      "turn out to be Hostile Animals?\"\n",
      "\n",
      "Piglet scratched his ear in a nice sort of way, and said that he had\n",
      "nothing to do until Friday, and would be delighted to come, in case it\n",
      "really _was_ a Woozle.\n",
      "\n",
      "\"You mean, in case it really is two Woozles,\" said Winnie-the-Pooh, and\n",
      "Piglet said that anyhow he had nothing to do until Friday. So off they\n",
      "went together.\n",
      "\n",
      "There was a small spinney of larch trees just here, and it seemed as if\n",
      "the two Woozles, if that is what they were, had been going round this\n",
      "spinney; so round this spinney went Pooh and Piglet after them; Piglet\n",
      "passing the time by telling Pooh what his Grandfather Trespassers W had\n",
      "done to Remove Stiffness after Tracking, and how his Grandfather\n",
      "Trespassers W had suffered in his later years from Shortness of Breath,\n",
      "and other matters of interest, and Pooh wondering what a Grandfather was\n",
      "like, and if perhaps this was Two Grandfathers they were after now, and,\n",
      "if so, whether he would be allowed to take one home and keep it, and\n",
      "what Christopher Robin would say. And still the tracks went on in front\n",
      "of them....\n",
      "\n",
      "Suddenly Winnie-the-Pooh stopped, and pointed excitedly in front of him.\n",
      "\"_Look!_\"\n",
      "\n",
      "\"_What?_\" said Piglet, with a jump. And then, to show that he hadn't\n",
      "been frightened, he jumped up and down once or twice more in an\n",
      "exercising sort of way.\n",
      "\n",
      "\"The tracks!\" said Pooh. \"_A third animal has joined the other two!_\"\n",
      "\n",
      "\"Pooh!\" cried Piglet. \"Do you think it is another Woozle?\"\n",
      "\n",
      "\"No,\" said Pooh, \"because it makes different marks. It is either Two\n",
      "Woozles and one, as it might be, Wizzle, or Two, as it might be, Wizzles\n",
      "and one, if so it is, Woozle. Let us continue to follow them.\"\n",
      "\n",
      "So they went on, feeling just a little anxious now, in case the three\n",
      "animals in front of them were of Hostile Intent. And Piglet wished very\n",
      "much that his Grandfather T. W. were there, instead of elsewhere, and\n",
      "Pooh thought how nice it would be if they met Christopher Robin suddenly\n",
      "but quite accidentally, and only because he liked Christopher Robin so\n",
      "much. And then, all of a sudden, Winnie-the-Pooh stopped again, and\n",
      "licked the tip of his nose in a cooling manner, for he was feeling more\n",
      "hot and anxious than ever in his life before. _There were four animals\n",
      "in front of them!_\n",
      "\n",
      "\"Do you see, Piglet? Look at their tracks! Three, as it were, Woozles,\n",
      "and one, as it was, Wizzle. _Another Woozle has joined them!_\"\n",
      "\n",
      "And so it seemed to be. There were the tracks; crossing over each other\n",
      "here, getting muddled up with each other there; but, quite\n"
     ]
    }
   ],
   "source": [
    "x = winnie_the_pooh.find('CHAPTER III')\n",
    "pooh_txt = winnie_the_pooh[x:x+5000]  # Extract the first 5000 characters of chapter 3\n",
    "print(pooh_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dae91135-57c1-413d-b608-9c6e1a0a3463",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context of Chapter III, the storyline is:\n",
      "\n",
      "Winnie-the-Pooh and Piglet go hunting for a Woozle, and they nearly catch it. Pooh is tracking the Woozle's paw-marks, and Piglet is excitedly following him. Pooh suggests that there may be two Woozles, and they continue to track them. Suddenly, they see that a third animal has joined the other two, and they are now tracking four animals in front of them. Pooh is excited and curious to know what the fourth animal is, and Piglet is feeling a bit anxious in case the animals are of hostile intent.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What is the storyline here? \"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always answer the questions based only on the context:{pooh_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e47a0878-33bb-40c5-abea-09dd3509d769",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context of Chapter III, the main character is Winnie-the-Pooh.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Who is the main character?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always answer the questions based only on the context:{pooh_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bb101247-2260-43d1-9310-4d2b3f7b61a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " At the end of Chapter III, Winnie-the-Pooh and Piglet finally catch up with the three animals they have been tracking. However, instead of being a single Woozle, they discover that there are actually four animals in front of them: three Woozles and one Wizzle. Pooh is excited to have found the third Woozle, but Piglet is feeling a bit anxious and scared, worried that the three animals might be of Hostile Intent.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What happens at the end?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always answer the questions based only on the context:{pooh_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc5493b8-fa69-4139-ae54-fe3480bef894",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are 10 question and answer pairs based on the context of Chapter III:\n",
      "\n",
      "1. Question: What is the Piglet's grandfather's name?\n",
      "Answer: Trespassers W.\n",
      "2. Question: What is the Piglet's grandfather's name short for?\n",
      "Answer: Trespassers William.\n",
      "3. Question: What is Winnie-the-Pooh's occupation?\n",
      "Answer: Hunting.\n",
      "4. Question: What is Winnie-the-Pooh thinking of?\n",
      "Answer: He is thinking of something else.\n",
      "5. Question: What do the tracks in front of Piglet and Winnie-the-Pooh look like?\n",
      "Answer: They look like paw-marks.\n",
      "6. Question: What do Piglet and Winnie-the-Pooh think they might be tracking?\n",
      "Answer: They think they might be tracking a Woozle.\n",
      "7. Question: What does Pooh say when Piglet asks him if he thinks it's a Woozle?\n",
      "Answer: He says, \"It may be.\"\n",
      "8. Question: What does Piglet say when Winnie-the-Pooh goes on tracking?\n",
      "Answer: He says, \"I'll come with you, Pooh, in case they turn out to be Hostile Animals.\"\n",
      "9. Question: What does Pooh say when Piglet asks him if he thinks there are two Woozles?\n",
      "Answer: He says, \"It is either Two Woozles and one, as it might be, Wizzle, or Two, as it might be, Wizzles and one, if so it is, Woozle.\"\n",
      "10. Question: How many animals are in front of Piglet and Winnie-the-Pooh?\n",
      "Answer: There are four animals in front of them: three Woozles and one Wizzle.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"Create {NQUESTIONS} question and answer pairs in the format: \"Question: ... Answer: ...\" \"\"\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always complete the tasks based only on the context:{pooh_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64acef7-d4fe-49b7-9ca4-b1bb94fdc4c2",
   "metadata": {},
   "source": [
    "### 3.3 Attention is all you need (by Vaswani et al)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b93e8343-42b5-4368-ad82-1b8cb90be303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "attention = nlp_helper.extract_pages('source_documents_dir/attention.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e59b3356-fa39-4ba7-849d-f1c5aeb98886",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"800\"\n",
       "            height=\"400\"\n",
       "            src=\"source_documents_dir/attention.pdf\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fa40ea2fb90>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attention_txt = '\\n\\n'.join(attention[1:3] + attention[9:10])  # We will use pages 1, 2 (for the intro), and 9 (for the conclusion)\n",
    "IFrame('source_documents_dir/attention.pdf', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470b58dc-40b9-4a21-9057-bfc4cdef2c20",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "65d11db2-8d9b-429e-9d29-4f3a1756de0c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The main gist of the paper is the introduction of a new neural network architecture called the Transformer, which is designed specifically for sequence modeling and transduction tasks, such as language modeling and machine translation. The Transformer relies entirely on attention mechanisms, rather than recurrent neural networks (RNNs), to draw global dependencies between input and output sequences. The authors claim that this approach allows for significantly more parallelization and achieves a new state of the art in translation quality after being trained for as little as twelve hours on eight P100 GPUs. The paper also compares the Transformer to other state-of-the-art models and shows that it outperforms them in various tasks.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What is the main gist of the paper?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always answer the questions based only on the context:{attention_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "49784f82-63f1-4f3e-8cef-39429dc89ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The problem being solved in the paper is the development of a new architecture for sequence modeling and transduction tasks, specifically machine translation and language modeling, that can achieve state-of-the-art results while also being more computationally efficient than existing approaches. The authors propose the Transformer model, which relies entirely on attention mechanisms rather than recurrent neural networks (RNNs) or convolutional neural networks (CNNs), to address this problem. They also compare the Transformer to other state-of-the-art models and show that it outperforms them in terms of both quality and computational efficiency.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What is the problem being solved?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always answer the questions based only on the context:{attention_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c63109d8-9340-48f6-a234-d17e62cdc8ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The conclusion of the paper is that the Transformer, a model architecture that relies entirely on attention mechanisms rather than recurrent sequences, achieves state-of-the-art results in sequence modeling and transduction tasks, including machine translation and constituency parsing. The Transformer outperforms previous models that use recurrent or convolutional layers, and can be trained significantly faster than those models. The authors are excited about the potential of attention-based models and plan to apply them to other tasks and investigate local, restricted attention mechanisms to handle large inputs and outputs.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What is the conclusion of the paper?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always answer the questions based only on the context:{attention_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ef8c05dc-a837-4454-b1aa-d13345dee2be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text will be split up into chunks of 1150 characters and summarized\n"
     ]
    }
   ],
   "source": [
    "chunk_size = len(attention_txt)//8\n",
    "print(f'The text will be split up into chunks of {chunk_size} characters and summarized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cf3771d6-11de-416e-be0c-ac91b84d93f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>Key Points</h4>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.  Recurrent neural networks (RNNs) are state-of-the-art for sequence modeling and transduction tasks, particularly language modeling and machine translation. RNNs factor computation along input/output positions, generating hidden states sequentially, which limits parallelization in training. Recent work has improved efficiency through factorization tricks and conditional computation, while also improving model performance.\n",
      "2.  Attention mechanisms are used in sequence modeling and translation tasks, but are often combined with recurrent networks. A new model called Transformer relies solely on attention to draw global dependencies, allowing for more parallelization and improved translation quality.\n",
      "3.  In the Transformer architecture, the self-attention mechanism is used to compute a representation of a sequence by relating different positions of the sequence. This is different from traditional recurrent neural networks, which use sequence-aligned recurrence. The Transformer's self-attention mechanism allows it to learn dependencies between distant positions, but it also reduces the effective resolution of the attention weights due to averaging. To counteract this effect, Multi-Head Attention is used.\n",
      "4.  The Transformer is a neural sequence transduction model that relies entirely on self-attention to compute representations of its input and output, without using sequence-aligned RNNs or convolution. It has an encoder-decoder structure, where the encoder maps an input sequence to continuous representations, and the decoder generates an output sequence one element at a time, using auto-regressive prediction. The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.\n",
      "5.  The Transformer architecture consists of an encoder and decoder, each composed of multiple identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a simple fully connected feed-forward network, with residual connections and layer normalization. The decoder also includes a third sub-layer for multi-head attention over the encoder output.\n",
      "6.  The Transformer model, a type of neural network architecture, is shown to generalize well to English constituency parsing tasks, achieving high F1 scores on the WSJ dataset.\n",
      "7.  In this paper, the authors present the Transformer, a new sequence transduction model that replaces recurrent layers with attention mechanisms. The Transformer outperforms previous models, including the Recurrent Neural Network Grammar, in both the semi-supervised and supervised settings, and achieves new state-of-the-art results on translation tasks.\n",
      "8.  The Transformer model is being extended to handle input and output modalities beyond text, with a focus on efficient and non-sequential generation. The code for training and evaluating the models is available at GitHub.\n"
     ]
    }
   ],
   "source": [
    "display(HTML('<h4>Key Points</h4>'))\n",
    "summary = []\n",
    "for i in range(8):\n",
    "    x0 = i*chunk_size\n",
    "    x1 = (i+1)*chunk_size\n",
    "    prompt=f\"\"\"Summarise the content within 30 words: {attention_txt[x0:x1]}\"\"\"\n",
    "    payload = {\n",
    "        \"inputs\": [[\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ]],\n",
    "        \"parameters\": {\"max_new_tokens\": 1000, \"temperature\": 0.01} \n",
    "    }\n",
    "    predictor.serializer = JSONSerializer()\n",
    "    response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "    line_summary = json.loads(response.decode())[0]['generation']['content']\n",
    "    summary.append(line_summary)\n",
    "    print(f'{i+1}. {line_summary}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02a1b27f-1f63-4da5-b3d0-a0a3c940f9fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are 10 question and answer pairs based on the provided text:\n",
      "\n",
      "1. Question: What is the main goal of the Transformer model?\n",
      "Answer: The main goal of the Transformer model is to reduce sequential computation in neural sequence transduction models.\n",
      "2. Question: How does the Transformer model differ from other sequence transduction models?\n",
      "Answer: The Transformer model differs from other sequence transduction models by relying entirely on an attention mechanism to draw global dependencies between input and output sequences, rather than using sequence-aligned recurrence or convolutional layers.\n",
      "3. Question: What is the purpose of the residual connections in the Transformer model?\n",
      "Answer: The residual connections in the Transformer model are used to facilitate the attention connections between sub-layers in each layer, allowing the model to learn more complex representations.\n",
      "4. Question: How does the Transformer model handle dependencies between distant positions?\n",
      "Answer: The Transformer model relies on attention mechanisms to handle dependencies between distant positions, allowing the model to weigh the importance of different positions in the input sequence.\n",
      "5. Question: What is the advantage of using self-attention in the Transformer model?\n",
      "Answer: The advantage of using self-attention in the Transformer model is that it allows the model to parallelize the computation of attention across different positions in the input sequence, making it more efficient and scalable.\n",
      "6. Question: How does the Transformer model generalize to English constituency parsing?\n",
      "Answer: The Transformer model generalizes well to English constituency parsing, as shown in Table 4, where it outperforms other sequence transduction models with the exception of the Recurrent Neural Network Grammar.\n",
      "7. Question: What is the difference between the Transformer model and the Berkeley Parser?\n",
      "Answer: The Transformer model differs from the Berkeley Parser in that it relies entirely on attention mechanisms, rather than recurrent or convolutional layers, to compute representations of the input and output sequences.\n",
      "8. Question: How does the Transformer model handle input and output modalities other than text?\n",
      "Answer: The Transformer model can be extended to handle input and output modalities other than text by using different types of attention mechanisms, such as visual or audio attention.\n",
      "9. Question: What is the advantage of using multi-head attention in the Transformer model?\n",
      "Answer: The advantage of using multi-head attention in the Transformer model is that it allows the model to learn multiple attention patterns simultaneously, improving the representation of the input sequence.\n",
      "10. Question: How does the Transformer model make generation less sequential?\n",
      "Answer: The Transformer model makes generation less sequential by using attention mechanisms to allow the model to attend to different positions in the input sequence simultaneously, rather than sequentially computing the attention for each position.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"Create {NQUESTIONS} question and answer pairs in the format: \"Question: ... Answer: ...\". eg.\"Question: What does Transformer rely on other than recurrent layers? Answer: attention mechanism \" \"\"\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always complete the tasks based ONLY on the context:{attention_txt}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 1024, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf13af1-b6be-4d34-9ad3-09f5cb70320d",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3.4 Australian Budget 2023-24 Overview (Medicare)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12091219-63d9-49f2-8f41-272be432731e",
   "metadata": {},
   "source": [
    "In this example, we look at the Australian Budget 2023-24 and we focus on the Medicare improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b747aa8a-bc64-45a6-ad40-37a30d8b483d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Historic investment in Medicare \n",
      "Strengthening Medicare\n",
      "Medicare is the foundation of Australia’s primary health care system. In this \n",
      "Budget, the Government is investing $5.7 billion over 5 years from 2022—23 to \n",
      "strengthen Medicare and make it cheaper and easier to see a doctor.\n",
      "The Strengthening Medicare package includes the largest investment in bulk \n",
      "billing incentives ever. The Government is...\n",
      "\n",
      "\n",
      "...llion over 4 years to establish the Primary Care and Midwifery \n",
      "Scholarships program, supporting registered nurses and midwives in \n",
      "post-graduate study to improve their skills \n",
      "• $31.6 million over 2 years for improved training arrangements for \n",
      "international medical students working rur al and remote locations.\n",
      "26 Strengthening MedicareStronger foundations for a better future   |   Budget 2023–24\n"
     ]
    }
   ],
   "source": [
    "# Extracting the pages from the Budget overview and work on the pages 24 to 27 (Medicare related)\n",
    "aus_budget_overview = nlp_helper.extract_pages('source_documents_dir/aus_budget_overview-2023-24.pdf')\n",
    "txt_aus_budget_overview_medicare = '\\n\\n'.join(aus_budget_overview[24:27])  # We will use pages 24 to 27. Those pages cover the Medicare budget.\n",
    "print(f'{txt_aus_budget_overview_medicare[:NCHARS]}...\\n\\n\\n...{txt_aus_budget_overview_medicare[-NCHARS:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28a3481-4f21-41f7-8656-521675f3b436",
   "metadata": {},
   "source": [
    "#### Question Answering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d7bbbc3-da27-42be-acf2-3eb3464ab793",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The Australian government is investing $5.7 billion over 5 years to strengthen Medicare and improve access to primary care, including bulk billing incentives and team-based care.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"CONTEXT: {txt_aus_budget_overview_medicare} Summarise the core information based on the above context. Summary should be within 30 words\"\"\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7e78e700-24fc-4642-91f8-5d4c4ce416b2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sure, here are 5 question and answer pairs based on the provided context:\n",
      "\n",
      "1. Question: What is the purpose of the Strengthening Medicare package?\n",
      "Answer: The Strengthening Medicare package aims to make it cheaper and easier for Australians to see a doctor by investing $5.7 billion over 5 years to strengthen Medicare and support primary care services.\n",
      "2. Question: What is the tripling of the bulk billing incentive for?\n",
      "Answer: The tripling of the bulk billing incentive is to support 11.6 million Australians to access a GP with no out-of-pocket costs, particularly families with children under 16 years, pensioners, and Commonwealth concession card holders.\n",
      "3. Question: What is the purpose of the investment in digital health?\n",
      "Answer: The investment in digital health is to modernise Australia's digital health platforms and provide health professionals with the digital and data tools needed to provide improved and more coordinated care, and to lift health outcomes.\n",
      "4. Question: How will the Primary Care and Midwifery Scholarships program support the health workforce?\n",
      "Answer: The Primary Care and Midwifery Scholarships program will support registered nurses and midwives in post-graduate study to improve their skills, which will help to strengthen the primary care system and provide better care for patients.\n",
      "5. Question: What is the purpose of connecting frequent hospital users to general practices?\n",
      "Answer: Connecting frequent hospital users to general practices is intended to provide comprehensive, multidisciplinary care in the community, rather than in hospitals, to improve health outcomes and reduce pressure on hospitals.\n"
     ]
    }
   ],
   "source": [
    "prompt=f\"\"\"Create 5 question and answer pairs in the format: \"Question number. Question: ... Answer: ...\" \"\"\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Always complete the tasks based ONLY on the context:{txt_aus_budget_overview_medicare}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 1024, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "910f00cc-2351-4832-b6c5-8ed803405c45",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " According to the Budget document, a Level B consultation refers to a telehealth general practice service that is between 6 and 20 minutes in length.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"What is a Level B consultation?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Provide brief answers based ONLY on the context:{txt_aus_budget_overview_medicare}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 1024, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7242da5d-d2a5-4661-ab97-bfa8c4993223",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Based on the context, the government is investing $5.7 billion over 5 years (from 2022-23 to 2026-27) to strengthen Medicare and make it cheaper and easier to see a doctor. Additionally, the government is investing $824.4 million in digital health to modernise Australia's digital health platforms and provide health professionals with the necessary digital and data tools to provide improved and more coordinated care.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"How much is the govermement investing?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Provide brief answers based ONLY on the context:{txt_aus_budget_overview_medicare}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 1024, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24c9a414-0dcd-4965-989c-6df103eb2dde",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, the government is investing in new services to help homeless people and culturally and linguistically diverse communities to access primary care. The budget provides $79.4 million over 4 years to support Primary Health Networks to commission allied health services to improve access to multidisciplinary care for people with chronic conditions in underserviced communities, including homeless people.\n"
     ]
    }
   ],
   "source": [
    "prompt=\"Is the governement helping the homeless people?\"\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": f'Provide brief answers based ONLY on the context:{txt_aus_budget_overview_medicare}'},\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 1024, \"temperature\": 0.01} \n",
    "}\n",
    "predictor.serializer = JSONSerializer()\n",
    "response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "answer = json.loads(response.decode())[0]['generation']['content']\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae610e3-1ecb-4dd8-94a4-137792b45b38",
   "metadata": {},
   "source": [
    "## 4. [Optional] LLM Demos for Education Part II\n",
    "\n",
    "In this section, we deploy a Gradio app that takes a URL as input, and allows us to answer questions based on the content of the web page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c2f91c-56e4-497d-a107-d2a2824ebff0",
   "metadata": {},
   "source": [
    "### 4.1 Gradio Demo App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "11f4c760-b110-41d5-818f-cf848da229fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "3c58ed7a-a92b-44c5-bfbb-74d7063b9bd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/pkg_resources/__init__.py:126: PkgResourcesDeprecationWarning: 4.0.0-unsupported is an invalid version and will not be supported in a future release\n",
      "  PkgResourcesDeprecationWarning,\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f803d584-df90-4e15-8923-0f19d7fb90cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def url2context(url):\n",
    "    paragraph_list = extract_paragraphs_from_html(\n",
    "        download_url_text(url)\n",
    "    )[1:11]  # We will skip the first paragraph, and take only 10 paragraphs\n",
    "    return '\\n\\n'.join(paragraph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d43bd2c0-b8d6-4b39-8a77-06851e28184f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/boto3/compat.py:82: PythonDeprecationWarning: Boto3 will no longer support Python 3.7 starting December 13, 2023. To continue receiving service updates, bug fixes, and security updates please upgrade to Python 3.8 or later. More information can be found here: https://aws.amazon.com/blogs/developer/python-support-policy-updates-for-aws-sdks-and-tools/\n",
      "  warnings.warn(warning, PythonDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://6fbbeae548e4a2888d.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://6fbbeae548e4a2888d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chatbot(prompt, temperature, max_length, url):\n",
    "    if url == \"\":\n",
    "        return generate_text_from_prompt(prompt, max_length, temperature)\n",
    "    else:\n",
    "        context = url2context(url)\n",
    "        payload = {\n",
    "            \"inputs\": [[\n",
    "                {\"role\": \"system\", \"content\": f'Provide brief answers based ONLY on the context:{context}'},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]],\n",
    "            \"parameters\": {\"max_new_tokens\": max_length, \"temperature\": temperature} \n",
    "        }\n",
    "        predictor.serializer = JSONSerializer()\n",
    "        response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "        answer = json.loads(response.decode())[0]['generation']['content']\n",
    "        return answer\n",
    "\n",
    "def summary(url):\n",
    "    content = url2context(url)\n",
    "    payload = {\n",
    "        \"inputs\": [[\n",
    "            {\"role\": \"system\", \"content\": f'Complete the tasks based ONLY on the context:{content}'},\n",
    "            {\"role\": \"user\", \"content\": \"Summarize the content.\"},\n",
    "        ]],\n",
    "        \"parameters\": {\"max_new_tokens\": 1024, \"temperature\": 0.01} \n",
    "    }\n",
    "    predictor.serializer = JSONSerializer()\n",
    "    response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "    summary_content = json.loads(response.decode())[0]['generation']['content']\n",
    "    payload = {\n",
    "        \"inputs\": [[\n",
    "            {\"role\": \"system\", \"content\": f'Complete the tasks based ONLY on the context:{content}'},\n",
    "            {\"role\": \"user\", \"content\": \"Find key words\\n Key Words:\"},\n",
    "        ]],\n",
    "        \"parameters\": {\"max_new_tokens\": 1024, \"temperature\": 0.01} \n",
    "    }\n",
    "    predictor.serializer = JSONSerializer()\n",
    "    response = predictor.predict(data=payload, custom_attributes='accept_eula=true')\n",
    "    key_words = json.loads(response.decode())[0]['generation']['content']\n",
    "    return f\"\"\"{summary_content}\\n\\nKey words: {key_words}\"\"\"  \n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Llama 2 Chatbot Demo\")\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            url = gr.Textbox(label=\"URL\", placeholder=\"Enter URL here\", lines=1, show_label=True,\n",
    "                             value=\"https://mmrjournal.biomedcentral.com/articles/10.1186/s40779-022-00416-w\"\n",
    "                             # value=\"https://k12.libretexts.org/Bookshelves/Science_and_Technology/Biology/03%3A_Genetics/3.14%3A_Human_Genome\"\n",
    "                            )\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            prompt = gr.Textbox(\n",
    "                label=\"Prompt\", placeholder=\"Enter your prompt here\", lines=3, show_label=True,\n",
    "                value=f\"How do mRNA vaccines work for pancreatic cancer treatment?\")\n",
    "            temperature = gr.Slider(label=\"Temperature\", minimum=0.0, maximum=1.0, value=0.5)\n",
    "            max_length = gr.Slider(label=\"Max Length\", minimum=20, maximum=400, value=100)\n",
    "        with gr.Column():\n",
    "            output = gr.Textbox(label=\"Output\", lines=10, show_label=True)\n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            submit_btn = gr.Button(\"Submit\")\n",
    "        with gr.Column():\n",
    "            summary_btn = gr.Button(\"Summary\")\n",
    "    submit_btn.click(\n",
    "        fn=chatbot,\n",
    "        inputs=[prompt, temperature, max_length, url],\n",
    "        outputs=output,\n",
    "        api_name=\"chatbot\",\n",
    "        queue=False\n",
    "    )\n",
    "    summary_btn.click(\n",
    "        fn=summary,\n",
    "        inputs=[url],\n",
    "        outputs=output,\n",
    "        api_name=\"summary\",\n",
    "        queue=False\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09014e18-33e5-4185-ab90-9957c25d1f39",
   "metadata": {
    "tags": []
   },
   "source": [
    "To completely shutdown SageMaker, go to File > Shut Down > Shutdown All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea71e5c-050b-4d52-b1ee-abd1ae047e19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
