{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is an attempt to fine tune the [music generator model](https://github.com/facebookresearch/audiocraft) developed by Meta.  This example references https://github.com/chavinlo/musicgen_trainer.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MUSICGEN is an autoregressive transformer-based decoder conditioned on text or melodic representation. In this example, we will only work on fine tuning the transformer with or without the text prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare audio files (.wav) and text prompts (.txt) and put them into the same directory to be used for training. The audio and text prompts should have corresponding file names and only differ by their extensions. Text prompts are optional and usually contain descriptions of the audio; the title of the audio/song is a good place to start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from audiocraft.models import MusicGen\n",
    "from transformers import get_scheduler,SchedulerType\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from audiocraft.modules.conditioners import ClassifierFreeGuidanceDropout\n",
    "import os\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Pytorch Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDataset(Dataset):\n",
    "    def __init__(self, data_dir, use_text_prompt=False):\n",
    "        self.data_dir = data_dir\n",
    "        self.data_map = []\n",
    "\n",
    "        dir_map = os.listdir(data_dir)\n",
    "        for d in dir_map:\n",
    "            name, ext = os.path.splitext(d)\n",
    "            if ext == \".wav\":\n",
    "                if not use_text_prompt:\n",
    "                    self.data_map.append({\"audio\": os.path.join(data_dir, d)})\n",
    "                    continue\n",
    "                if os.path.exists(os.path.join(data_dir, name + \".txt\")):\n",
    "                    self.data_map.append(\n",
    "                        {\n",
    "                            \"audio\": os.path.join(data_dir, d),\n",
    "                            \"text_prompt\": os.path.join(data_dir, name + \".txt\"),\n",
    "                        }\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"No text_prompt file for {name}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_map)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data_map[idx]\n",
    "        audio = data[\"audio\"]\n",
    "        text_prompt = data.get(\"text_prompt\", \"\")\n",
    "\n",
    "        return audio, text_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the audio file by \n",
    "1. resampling the audio file to match the model's sample rate\n",
    "2. convert the audio to monophonic\n",
    "3. random sampling of the audio file to extract different sections of the music\n",
    "4. compress the model to output vectors based on the number of codebooks used and its associated cardinality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_audio(audio_path, model: MusicGen, duration: int = 30):\n",
    "    wav, sr = torchaudio.load(audio_path)\n",
    "    # resample wav to model's sample rate\n",
    "    wav = torchaudio.functional.resample(wav, sr, model.sample_rate)\n",
    "    # convert to monophonic audio\n",
    "    wav = wav.mean(dim=0, keepdim=True)\n",
    "    if wav.shape[1] < model.sample_rate * duration:\n",
    "        return None\n",
    "    # end index for sampling\n",
    "    end_sample = int(model.sample_rate * duration)\n",
    "    # randomize start index for sampling\n",
    "    start_sample = random.randrange(0, max(wav.shape[1] - end_sample, 1))\n",
    "    wav = wav[:, start_sample : start_sample + end_sample]\n",
    "\n",
    "    assert wav.shape[0] == 1 # ensure monophonic audio\n",
    "\n",
    "    wav = wav.cuda()\n",
    "    wav = wav.unsqueeze(1)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        gen_audio = model.compression_model.encode(wav) # vector quantization (1,num_codebooks,codebook cardinality)\n",
    "\n",
    "    codes, scale = gen_audio\n",
    "\n",
    "    assert scale is None\n",
    "\n",
    "    return codes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode the audio based on the cardinality of the codebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(tensor, num_classes=2048):\n",
    "    shape = tensor.shape\n",
    "    one_hot = torch.zeros((shape[0], shape[1], num_classes))\n",
    "\n",
    "    for i in range(shape[0]):\n",
    "        for j in range(shape[1]):\n",
    "            index = tensor[i, j].item()\n",
    "            one_hot[i, j, index] = 1\n",
    "\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'input your dataset directory here'\n",
    "model_id = 'small'\n",
    "lr = 1e-5\n",
    "epochs =  100\n",
    "use_text_prompt = False\n",
    "grad_acc = 1\n",
    "weight_decay = 1e-5\n",
    "warmup_steps = 10\n",
    "batch_size = 1\n",
    "use_cfg = True # classifier free guidance dropout\n",
    "save_step = None\n",
    "save_models = False if save_step is None else True\n",
    "save_path = \"./models/\"\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n",
    "1. Get the pretrain model\n",
    "2. Cast the precision of the language model to float32 - using a lower precision will result in null loss values\n",
    "3. Load dataset and create data loader\n",
    "4. Initialise model optimizer, learning rate scheduler and loss function\n",
    "5. Train the language model by calculating the loss of the output with respect to the input code. No shifting of logits is required as mentioned in the comments of compute_prediction in lm.py. The loss is calculated only for the first codebook which is the most important one. The other codebooks encodes the quantization error left by the first codebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MusicGen.get_pretrained(model_id)\n",
    "model.lm = model.lm.to(torch.float32)  # loss will result in na with lower precision\n",
    "model.lm.cuda()\n",
    "\n",
    "if not use_text_prompt: # remove cross attention layers if not using text_prompt\n",
    "    for layer in model.lm.transformer.layers:\n",
    "        layer.cross_attention = None\n",
    "\n",
    "dataset = AudioDataset(dataset_path, use_text_prompt=use_text_prompt)\n",
    "train_dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model.lm.train()\n",
    "\n",
    "optimizer = AdamW(\n",
    "    model.lm.transformer.parameters(), # only train the transformer\n",
    "    lr=lr,\n",
    "    betas=(0.9, 0.95),\n",
    "    weight_decay=weight_decay,\n",
    ")\n",
    "\n",
    "scheduler = get_scheduler(\n",
    "    SchedulerType.COSINE,\n",
    "    optimizer,\n",
    "    warmup_steps,\n",
    "    int(epochs * len(train_dataloader) / grad_acc),\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "current_step = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch_idx, (audio, text_prompt) in enumerate(train_dataloader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        all_codes = []\n",
    "        if use_text_prompt:\n",
    "            texts = []\n",
    "\n",
    "        # where audio and text_prompt are just paths\n",
    "        for inner_audio, l in zip(audio, text_prompt):\n",
    "            inner_audio = preprocess_audio(inner_audio, model)  # returns tensor\n",
    "            if inner_audio is None:\n",
    "                continue\n",
    "\n",
    "            if use_cfg:\n",
    "                codes = torch.cat([inner_audio, inner_audio], dim=0)\n",
    "            else:\n",
    "                codes = inner_audio\n",
    "\n",
    "            all_codes.append(codes)\n",
    "            if use_text_prompt:\n",
    "                texts.append(open(l, \"r\").read().strip())\n",
    "\n",
    "        if use_text_prompt:\n",
    "            attributes, _ = model._prepare_tokens_and_attributes(texts, None)\n",
    "            conditions = attributes\n",
    "            if use_cfg:\n",
    "                null_conditions = ClassifierFreeGuidanceDropout(p=1.0)(conditions)\n",
    "                conditions = conditions + null_conditions\n",
    "            tokenized = model.lm.condition_provider.tokenize(conditions)\n",
    "            cfg_conditions = model.lm.condition_provider(tokenized)\n",
    "            condition_tensors = cfg_conditions\n",
    "\n",
    "        if len(all_codes) == 0:\n",
    "            continue\n",
    "\n",
    "        codes = torch.cat(all_codes, dim=0)\n",
    "        \n",
    "        # run ops in mixed precision\n",
    "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "            if use_text_prompt:\n",
    "                lm_output = model.lm.compute_predictions(\n",
    "                    codes=codes, conditions=[], condition_tensors=condition_tensors\n",
    "                )\n",
    "            else:\n",
    "                lm_output = model.lm.compute_predictions(\n",
    "                    codes=codes, conditions=[]\n",
    "                )\n",
    "\n",
    "            codes = codes[0]\n",
    "            logits = lm_output.logits[0]\n",
    "            mask = lm_output.mask[0]\n",
    "\n",
    "            codes = one_hot_encode(codes, num_classes=2048)\n",
    "\n",
    "            codes = codes.cuda()\n",
    "            logits = logits.cuda()\n",
    "            mask = mask.cuda()\n",
    "\n",
    "            mask = mask.view(-1)\n",
    "            masked_logits = logits.view(-1, 2048)[mask]\n",
    "            masked_codes = codes.view(-1, 2048)[mask]\n",
    "\n",
    "            loss = criterion(masked_logits, masked_codes)\n",
    "            \n",
    "        current_step += 1 / grad_acc\n",
    "        loss.backward()\n",
    "\n",
    "        print(f\"Epoch: {epoch}/{epochs}, Batch: {batch_idx}/{len(train_dataloader)}, Loss: {loss.item()}\")\n",
    "\n",
    "        if batch_idx % grad_acc != grad_acc - 1:\n",
    "            continue\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.lm.parameters(), 0.5)\n",
    "    \n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        if save_models:\n",
    "            if (current_step == int(current_step) and int(current_step) % save_step == 0):\n",
    "                torch.save(model.lm.state_dict(), f\"{save_path}/lm_{current_step}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'input your text prompt here'\n",
    "duration = 30\n",
    "sample_loops = 4\n",
    "use_sampling = 1\n",
    "two_step_cfg = 0\n",
    "top_k = 250\n",
    "top_p = 0.0\n",
    "temperature = 1.0\n",
    "cfg_coef = 3.0\n",
    "save_path = 'output.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_text_prompt:\n",
    "    attributes, prompt_tokens = model._prepare_tokens_and_attributes([prompt], None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generation_params = {\n",
    "    'max_gen_len': int(duration * model.frame_rate),\n",
    "    'use_sampling': use_sampling,\n",
    "    'temp': temperature,\n",
    "    'top_k': top_k,\n",
    "    'top_p': top_p,\n",
    "    'cfg_coef': cfg_coef,\n",
    "    'two_step_cfg': two_step_cfg,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate tokens autoregressively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = []\n",
    "model.lm.eval()\n",
    "for _ in trange(sample_loops):\n",
    "    with model.autocast:\n",
    "        if use_text_prompt:\n",
    "            gen_tokens = model.lm.generate(prompt_tokens, attributes, callback=None, **model.generation_params)\n",
    "            total.append(gen_tokens[..., prompt_tokens.shape[-1] if prompt_tokens is not None else 0:])\n",
    "            prompt_tokens = gen_tokens[..., -gen_tokens.shape[-1] // 2:]\n",
    "        else:\n",
    "            gen_tokens = model.lm.generate(None, None, callback=None, **model.generation_params)\n",
    "            total.append(gen_tokens)\n",
    "gen_tokens = torch.cat(total, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert gen_tokens.dim() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct audio representation from generated codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    gen_audio = model.compression_model.decode(gen_tokens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_audio = gen_audio.cpu()\n",
    "torchaudio.save(save_path, gen_audio[0], model.sample_rate)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
